{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - SKLearn, TensorFlow, Keras, PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Harry Yau\n",
    "\n",
    "Date: September 23, 2019\n",
    "\n",
    "Updated: September 24, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This goal of this notebook is to compute a multi-class Logistic Regression using different frameworks. The frameworks that will be investigated are:\n",
    "- Sklearn\n",
    "- Statsmodel\n",
    "- TensorFlow 1.0\n",
    "- Keras\n",
    "- PyTorch\n",
    "- TensorFlow 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equations pertaining to logistic regression:\n",
    "$$ z = W^Tx + b$$\n",
    "$$ \\hat{y} = \\sigma(W^Tx + b)$$\n",
    "$$ \\sigma(z) = \\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multi-classification problem, the softmax function will be used for the activation function in the output layer.\n",
    "$$Softmax(x_i)=\\frac{e^{x_i}}{\\sum_j e^{x_j}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function that will be used will be the cross-entropy loss.\n",
    "$$H(y, \\hat{y}) = - \\sum_i y_i log(\\hat{y_i})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numpy array is forced to have dtype np.float32 so that pytorch will accept these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(load_iris().data, dtype=np.float32)\n",
    "y = np.array(load_iris().target, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the data so that it helps with convergence when running gradient descent in TensorFlow, Keras and Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_scaler = StandardScaler()\n",
    "X_scaled = X_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_iris().feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_iris().target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardized Settings for TensorFlow, Keras and PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 4        # takes variable 'x' 4 input variables, due to 4 columns\n",
    "output_dim = 3       # takes variable 'y' 3 output variables, due to 3 output categorizations\n",
    "learning_rate = 0.01\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Included a large regularization value C to offset the L2 regularization that is default in SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9866666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_model = LogisticRegression(C=1e8, max_iter=epochs, solver='lbfgs', multi_class='multinomial', n_jobs=-1, verbose=0)\n",
    "log_model.fit(X, y.squeeze())\n",
    "probas = log_model.predict_log_proba(X)\n",
    "predicted = np.argmax(probas, 1)\n",
    "print(\"Accuracy Score:\", accuracy_score(y, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statsmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters method and maxiter are tuneable in the fit argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.9866666666666667\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm # For constants\n",
    "from statsmodels.discrete.discrete_model import MNLogit\n",
    "\n",
    "X_c = sm.add_constant(X)\n",
    "log_model_sm = sm.MNLogit(y, X_c).fit(method='lbfgs', maxiter=epochs) #Fit the model\n",
    "predicted = np.argmax(log_model_sm.predict(X_c), 1)\n",
    "print(\"Accuracy Score:\", accuracy_score(y, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In TensorFlow there is no loss function specifically for cross-entropy. However there is a loss function that combines both the Softmax and the cross-entropy loss function.\n",
    "\n",
    "There are two path ways to implement this loss function. However, this will influence how the placeholder and cost function will be chosen.\n",
    "- With One-hot\n",
    "    - Placeholder: Ensure the shape to be output of dimension `[None, output_dim]`. output_dim being the number of classes\n",
    "        - `y_tf = tf.placeholder(tf.float32, shape=[None, output_dim], name='y')`\n",
    "    - feed_dict: Ensure that you pass data as a one-hot with dtype float32. Use `pd.getdummies()` to one-hot\n",
    "        - `y_onehot = pd.get_dummies(y.squeeze())`\n",
    "        - `y_onehot = np.array(y_onehot, dtype='float32')`\n",
    "    - `tf.losses.softmax_cross_entropy()`, this uses the function below\n",
    "    - `tf.nn.softmax_cross_entropy_with_logits_v2()` - must wrap this with `tf.reduce_mean()`\n",
    "- Without one-hot\n",
    "    - Placeholder: Ensure the shape to be output of dimension `[None, 1]`\n",
    "        - `y_tf = tf.placeholder(tf.float32, shape=[None, 1], name='y')`\n",
    "    - feed_dict: Ensure that you pass a 1-D array with dtype int32\n",
    "        - `y_sparse = np.array(y, dtype='int32').reshape(-1,1)`\n",
    "    - Use the loss functions with sparse in the name.\n",
    "        - `tf.losses.sparse_softmax_cross_entropy()`\n",
    "        - `tf.nn.sparse_softmax_cross_entropy_with_logits()` - must wrap this with `tf.reduce_mean()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('Version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will demonstrate the version with one-hot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to one-hot the y labels for `tf.losses.softmax_cross_entropy` to work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_onehot = pd.get_dummies(y.squeeze())\n",
    "y_onehot = np.array(y_onehot, dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the placeholder for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tf = tf.placeholder(tf.float32, shape=[None, input_dim], name='X')\n",
    "y_tf = tf.placeholder(tf.float32, shape=[None, output_dim], name='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create variables for W and b. Initializing W with Xavier/Glorot Initialization and b with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yauha\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "W_tf = tf.Variable(tf.glorot_uniform_initializer()((input_dim, output_dim)), dtype=tf.float32, name='W')\n",
    "b_tf = tf.Variable(tf.zeros_initializer()((output_dim)), dtype=tf.float32, name='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the y predicted equation. \n",
    "\n",
    "Note: There is need to wrap the equation with `tf.nn.softmax()` because the loss function calculates the softmax. Without the wrapper is the logit, and this is passed into the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yauha\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "#WX + b\n",
    "y_pred_tf = tf.add(tf.matmul(X_tf, W_tf), b_tf)\n",
    "\n",
    "#The cost function\n",
    "cost_tf = tf.losses.softmax_cross_entropy(onehot_labels=y_onehot, logits=y_pred_tf)\n",
    "\n",
    "#The optimizer\n",
    "optimizer_tf = tf.train.AdamOptimizer(learning_rate).minimize(cost_tf)\n",
    "\n",
    "#Global Variables Initilizer -- Mandatory\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the computation graph. For the output, run an argmax function `tf.argmax()` on the output results to get the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 : cost = 0.38219124\n",
      "Epoch 100 : cost = 0.29272097\n",
      "Epoch 150 : cost = 0.23531395\n",
      "Epoch 200 : cost = 0.19645964\n",
      "Epoch 250 : cost = 0.16920346\n",
      "Epoch 300 : cost = 0.14931485\n",
      "Epoch 350 : cost = 0.13427193\n",
      "Epoch 400 : cost = 0.12254125\n",
      "Epoch 450 : cost = 0.11315745\n",
      "Epoch 500 : cost = 0.105489746\n",
      "Epoch 550 : cost = 0.09911152\n",
      "Epoch 600 : cost = 0.09372509\n",
      "Epoch 650 : cost = 0.08911712\n",
      "Epoch 700 : cost = 0.08513096\n",
      "Epoch 750 : cost = 0.08164919\n",
      "Epoch 800 : cost = 0.07858214\n",
      "Epoch 850 : cost = 0.07586017\n",
      "Epoch 900 : cost = 0.07342846\n",
      "Epoch 950 : cost = 0.07124324\n",
      "Epoch 1000 : cost = 0.069269195\n"
     ]
    }
   ],
   "source": [
    "#The Tensorflow Session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        feed_dict = feed_dict={X_tf: X_scaled, y_tf: y_onehot}\n",
    "        \n",
    "        #Run the optimizer at every epoch\n",
    "        sess.run(optimizer_tf, feed_dict)\n",
    "            \n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            #Calculate the cost at every epoch\n",
    "            print(\"Epoch\", (epoch + 1), \": cost =\", cost_tf.eval(feed_dict)) \n",
    "    \n",
    "    # Store values\n",
    "    predicted = sess.run(tf.argmax(y_pred_tf, 1), feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Keras the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 15\n",
      "Trainable params: 15\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(output_dim, activation='softmax', input_shape=(input_dim,)),\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x=X_scaled, y=y, epochs=epochs, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "acc = model.evaluate(X_scaled, y, verbose=0)[1]\n",
    "print('Accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel_PyT(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegressionModel_PyT, self).__init__()\n",
    "        self.W = torch.nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty(input_dim, output_dim)))\n",
    "        self.b = torch.nn.Parameter(torch.zeros(output_dim))\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.matmul(x, self.W) + self.b\n",
    "        return out            \n",
    "\n",
    "class LogisticRegressionModel_PyT_Layers(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegressionModel_PyT_Layers, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionModel_PyT(input_dim, output_dim)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    \n",
    "criterion = torch.nn.CrossEntropyLoss() #This calculates both nn.LogSoftmax and nn.NLLLoss\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, loss 0.3800296485424042\n",
      "epoch 100, loss 0.2913043797016144\n",
      "epoch 150, loss 0.2375224232673645\n",
      "epoch 200, loss 0.19984683394432068\n",
      "epoch 250, loss 0.1726667284965515\n",
      "epoch 300, loss 0.15250207483768463\n",
      "epoch 350, loss 0.1371167004108429\n",
      "epoch 400, loss 0.12506866455078125\n",
      "epoch 450, loss 0.11541444063186646\n",
      "epoch 500, loss 0.10752244293689728\n",
      "epoch 550, loss 0.10095907747745514\n",
      "epoch 600, loss 0.09541921317577362\n",
      "epoch 650, loss 0.0906829759478569\n",
      "epoch 700, loss 0.08658842742443085\n",
      "epoch 750, loss 0.08301404863595963\n",
      "epoch 800, loss 0.07986698299646378\n",
      "epoch 850, loss 0.07707515358924866\n",
      "epoch 900, loss 0.07458183169364929\n",
      "epoch 950, loss 0.07234172523021698\n",
      "epoch 1000, loss 0.0703183189034462\n"
     ]
    }
   ],
   "source": [
    "#Convert to Variable\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    X_data = torch.from_numpy(X_scaled).cuda()\n",
    "    y_data = torch.tensor(y.squeeze(), dtype=torch.long).cuda()\n",
    "else:\n",
    "    X_data = torch.from_numpy(X_scaled)\n",
    "    y_data = torch.tensor(y.squeeze(), dtype=torch.long)\n",
    "\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, \n",
    "    # dont want to cummulate gradients\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    # Get output from the model, given the inputs\n",
    "    y_pred = model(X_data)\n",
    "    # Get loss for the predicted output\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    # Get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimiser.step()\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print('epoch {}, loss {}'.format(epoch + 1, loss.item()))    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    if torch.cuda.is_available():\n",
    "        predicted = model(torch.from_numpy(X_scaled).cuda())\n",
    "        _, predicted = torch.max(predicted.data, 1)\n",
    "        predicted = predicted.cpu().data.numpy()\n",
    "    else:\n",
    "        predicted = model(torch.from_numpy(X_scaled))\n",
    "        _, predicted = torch.max(predicted.data, 1)\n",
    "        predicted = predicted.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9733333333333334"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: The environment has been switched at this point of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 2.0.0-rc2\n",
      "Version: 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('Version:', tf.__version__)\n",
    "print('Version:', tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the model.\n",
    "\n",
    "Differences from TensorFlow 1.0:\n",
    "- `tf.placeholders()`, `tf.Session()` has been removed\n",
    "- `tf.glorot_uniform_initializer()` is gone, and has become `tf.keras.initializers.GlorotUniform()`\n",
    "- Loss functions are streamlined. Must have a softmax layer and use a Cross-Entropy loss function. There are no loss function that calculates both the softmax and cross-entropy anymore.\n",
    "\n",
    "Alternatively, the model class can utilize the layers in Keras. This is analogous to PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel_TF2(tf.keras.Model):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegressionModel_TF2, self).__init__()\n",
    "        self.W = tf.Variable(tf.keras.initializers.GlorotUniform()((input_dim, output_dim)), dtype=tf.float32, name='W')\n",
    "        self.b = tf.Variable(tf.zeros_initializer()((output_dim)), dtype=tf.float32, name='b')\n",
    "    \n",
    "    #In PyTorch the function will be 'forward' instead.\n",
    "    def call(self, x):\n",
    "        x = tf.matmul(x, self.W) + self.b\n",
    "        out = tf.keras.activations.softmax(x)\n",
    "        return out\n",
    "    \n",
    "class LogisticRegressionModel_TF2_Layers(tf.keras.Model):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegressionModel_TF2_Layers, self).__init__()\n",
    "        self.d1 = tf.keras.layers.Dense(output_dim, activation='softmax', input_dim=(input_dim,))\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.d1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionModel_TF2(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_loss(model, inputs, targets):\n",
    "    predictions = model(inputs)\n",
    "    return criterion(targets, predictions)\n",
    "\n",
    "@tf.function\n",
    "def grad(model, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs)\n",
    "        loss_value = criterion(targets, predictions)\n",
    "        \n",
    "    #This calculates teh gradient\n",
    "    return tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "criterion = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Loss: 0.5457875728607178\n",
      "Epoch 100, Loss: 0.37291446328163147\n",
      "Epoch 150, Loss: 0.30826982855796814\n",
      "Epoch 200, Loss: 0.2659653127193451\n",
      "Epoch 250, Loss: 0.23418797552585602\n",
      "Epoch 300, Loss: 0.20936702191829681\n",
      "Epoch 350, Loss: 0.18952356278896332\n",
      "Epoch 400, Loss: 0.17334893345832825\n",
      "Epoch 450, Loss: 0.15994036197662354\n",
      "Epoch 500, Loss: 0.14866115152835846\n",
      "Epoch 550, Loss: 0.13905280828475952\n",
      "Epoch 600, Loss: 0.13077835738658905\n",
      "Epoch 650, Loss: 0.1235850527882576\n",
      "Epoch 700, Loss: 0.11727989464998245\n",
      "Epoch 750, Loss: 0.11171285063028336\n",
      "Epoch 800, Loss: 0.10676562041044235\n",
      "Epoch 850, Loss: 0.1023435965180397\n",
      "Epoch 900, Loss: 0.09837018698453903\n",
      "Epoch 950, Loss: 0.09478282183408737\n",
      "Epoch 1000, Loss: 0.09152979403734207\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    #Backward propagation\n",
    "    grads = grad(model, X_scaled, y)\n",
    "    \n",
    "    #update parameters\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    if(epoch+1) % 50 == 0:\n",
    "        template = 'Epoch {}, Loss: {}'\n",
    "        print(template.format(epoch+1, compute_loss(model, X_scaled, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9733333333333334"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = tf.argmax(model(X_scaled), 1)\n",
    "accuracy_score(y, predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
